{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gekitai with Reiforcement Lerning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- This notebook will walk through the various stages of the implementation of a custom [OpenAI gym](https://www.gymlibrary.ml) for the gekitai game.\n",
    "\n",
    "- The gekitai rules are available [here](https://boardgamegeek.com/boardgame/295449/gekitai)\n",
    "\n",
    "## Contributors\n",
    "\n",
    "- [Jo√£o Sousa](mailto:up201904739@edu.fc.up.pt)\n",
    "- [Miguel Rodrigues](mailto:up201906042@edu.fe.up.pt)\n",
    "- [Ricardo Ferreira](mailto:up201907835@edu.fe.up.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gekitai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking if the developed gym follows the specification of the OpenAI gym\n",
    "from gym.utils.env_checker import check_env\n",
    "\n",
    "env = gym.make('gekitai-v0')\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the environment\n",
    "\n",
    "Below there is a small test just to make sure is up and running.\n",
    "In the snippet below nothing fancy happens, the step of the environment is by taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = gym.make('gekitai-v0', render_mode='human')\n",
    "    observation = env.reset()\n",
    "\n",
    "    episodes = 5\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            env.render(mode='human')\n",
    "\n",
    "        print(info)\n",
    "        observation = env.reset()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "In the next section we will discuss and view how can an agent learn to play the gekitai using our custom developed environment. \n",
    "\n",
    "### Considerations\n",
    "\n",
    "Once we are using [OpenAI gym](https://www.gymlibrary.ml/), we had to face a challenge regarding single vs. multi agent environments. The fact is that gym's interface is targeted towards single-agent environments meant that we were required to adapt our 2-player board game, hence a multi-agent environment, to a single-agent environment. \n",
    "\n",
    "For that, our `step()` function executes a move for both the agent and its opponent. The way that we use for generate a move for the open is pretty simple - we choose a random action but with a small catch of insider spaces (more valuable) have a higher probability of being chosen. This simplicity comes from the fact that the usage of more complex algorithms could take way too long to compute each step, what would translate in even longer times when training our RL models.\n",
    "\n",
    "Another relevant aspect is the choice of the training algorithms. Not all RL algorithms work with out environment due to the fact that `action_space` is of type `Discrete()` and `observation_space` is of type `Box()` which means it is continuous.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "Taking into account the considerations stated above some of the algorithms compatible with our environment are:\n",
    "\n",
    "- DQN\n",
    "- PPO\n",
    "- A2C\n",
    "\n",
    "The implementation for those algorithms will be provided by the Stable Baselines3 library, since it provides a very friendly and easy-to-use API, very handy for solving all sorts of tasks related to RL. The documentation for the library can be found [here](https://stable-baselines3.readthedocs.io/en/master/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    env = gym.make('gekitai-v0', render_mode='human')\n",
    "    observation = env.reset()\n",
    "\n",
    "    episodes = 5\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _states = model.predict(observation, deterministic=True)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            env.render(mode='human')\n",
    "\n",
    "        print(info)\n",
    "        observation = env.reset()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs setup for visualization through TensorBoard\n",
    "import os\n",
    "\n",
    "logs_dir = f'logs'\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparemeters\n",
    "\n",
    "Hypermeters allow to perform fine tunning on the different algorithms used below.\n",
    "\n",
    "In the next cell are defined the values for the discount factor and the learning rate. However, Stable Baselines3 allows a deeper fine tunning for the used algorithms. Please refer to Stable Baselines3 [documentation](https://stable-baselines3.readthedocs.io/en/master/) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99  # Discount Factor\n",
    "learning_rate = 7e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN (Deep Q-Network)\n",
    "\n",
    "The DQN algorithm is based in the Q-learning algorithm. Basically, the Q-table which store the Q-values for each pair `(state, action)` from the latter is substituted by a a neural network which is trained to estimate that same Q-value, in other words, Q-learning is for a discrete `observation_space` what DQN is for a continuous `observation_space`.\n",
    "\n",
    "Both DQN and Q-learning have the charateristic of being off-policy, meaning that the behaviour of the agent is completely independent from the produced estimates for the value function.<sup>[1]</sup>\n",
    "\n",
    "[1]: https://paperswithcode.com/method/dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "dqn_models_dir = 'models/dqn'\n",
    "if not os.path.exists(dqn_models_dir):\n",
    "    os.makedirs(dqn_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 0.1\n",
    "env = gym.make('gekitai-v0', render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "dqn_model = DQN('MlpPolicy', env, learning_rate=learning_rate, gamma=gamma,  verbose=1, tensorboard_log=logs_dir)\n",
    "dqn_model.learn(total_timesteps=1e6, reset_num_timesteps=False, tb_log_name=f'dqn_v{version}')\n",
    "dqn_model.save(f'{dqn_models_dir}/gekitai_dnq_v{version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the model DQN learned to play the game\n",
    "test_model(dqn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO (Proximal Policy Optimization)\n",
    "\n",
    "The PPO<sup>[2]</sup> algorithm is an algorithm developed at OpenAI that tries to find a balance between different aspects as **ease of tunning**, **sample efficiency** and **code complexity**. It works with an **on-line** basis, meaning that unlike DQN there is no replay buffer where the agent can learn from current and past actions but rather the agent only learns from what the current action is and it only processes that same action once for the entire lifespan of an episode - in order to update the model's policy gradient.\n",
    "\n",
    "[2]: https://www.youtube.com/watch?v=5P7I-xPq8u8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "ppo_models_dir = 'models/ppo'\n",
    "if not os.path.exists(ppo_models_dir):\n",
    "    os.makedirs(ppo_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 0.1\n",
    "env = gym.make('gekitai-v0', render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "ppo_model = PPO('MlpPolicy', env, learning_rate=learning_rate, gamma=gamma, verbose=1, tensorboard_log=logs_dir)\n",
    "ppo_model.learn(total_timesteps=1e6, reset_num_timesteps=False, tb_log_name=f'ppo_v{version}')\n",
    "ppo_model.save(f'{ppo_models_dir}/gekitai_ppo_v{version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the PPO model learned to play the game\n",
    "test_model(ppo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C (Advantage Actor Critic)\n",
    "\n",
    "A2C is a policy gradient algortihm and it is part of the on-policy family of RL algorithms<sup>[3]</sup>. It consists of 2 networks the **actor** and the **critic** who work together in order to solve a particular problem based on an advantage function which calculates the agent's temporal difference error.\n",
    "\n",
    "This means that the A2C algorithm works on a temporal difference learning paradigm by using error prediction, very similar to how the human brain also learns new things<sup>[4]</sup>.\n",
    "\n",
    "[3]: https://medium.com/deeplearningmadeeasy/advantage-actor-critic-a2c-implementation-944e98616b\n",
    "[4]: https://towardsdatascience.com/advantage-actor-critic-tutorial-mina2c-7a3249962fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "a2c_models_dir = 'models/a2c'\n",
    "if not os.path.exists(ppo_models_dir):\n",
    "    os.makedirs(ppo_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 0.1\n",
    "env = gym.make('gekitai-v0', render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "a2c_model = A2C('MlpPolicy', env, learning_rate=learning_rate, gamma=gamma, verbose=1, tensorboard_log=logs_dir)\n",
    "a2c_model.learn(total_timesteps=1e6, reset_num_timesteps=False, tb_log_name=f'a2c_v{version}')\n",
    "a2c_model.save(f'{a2c_models_dir}/gekitai_a2c_v{version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the A2C model learned to play the game\n",
    "test_model(a2c_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
