{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Gekitai with Reiforcement Lerning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- This notebook will walk through the various stages of the implementation of a custom [OpenAI gym](https://www.gymlibrary.ml) for the gekitai game.\n",
    "\n",
    "- The gekitai rules are available [here](https://boardgamegeek.com/boardgame/295449/gekitai)\n",
    "\n",
    "## Contributors\n",
    "\n",
    "- [Jo√£o Sousa](mailto:up201904739@edu.fc.up.pt)\n",
    "- [Miguel Rodrigues](mailto:up201906042@edu.fe.up.pt)\n",
    "- [Ricardo Ferreira](mailto:up201907835@edu.fe.up.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gekitai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking if the developed gym follows the specification of the OpenAI gym\n",
    "from gym.utils.env_checker import check_env\n",
    "\n",
    "env = gym.make('gekitai-v0')\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the environment\n",
    "\n",
    "Below there is a small test just to make sure is up and running.\n",
    "In the snippet below nothing fancy happens, the step of the environment is by taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('gekitai-v0', render_mode='human')\n",
    "observation = env.reset()\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "    \n",
    "        env.render(mode='human')\n",
    "    \n",
    "    print(info)\n",
    "    observation = env.reset()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "In the next section we will discuss and view how can an agent learn to play the gekitai using our custom developed environment. \n",
    "\n",
    "### Considerations\n",
    "\n",
    "Once we are using [OpenAI gym](https://www.gymlibrary.ml/), we had to face a challenge regarding single vs. multi agent environments. The fact is that gym's interface is targeted towards single-agent environments meant that we were required to adapt our 2-player board game, hence a multi-agent environment, to a single-agent environment. \n",
    "\n",
    "For that, our `step()` function executes a move for both the agent and its opponent. The way that we use for generate a move for the open is pretty simple - we choose a random action but with a small catch of insider spaces (more valuable) have a higher probability of being chosen. This simplicity comes from the fact that the usage of more complex algorithms could take way too long to compute each step, what would translate in even longer times when training our RL models.\n",
    "\n",
    "Another relevant aspect is the choice of the training algorithms. Not all RL algorithms work with out environment due to the fact that `action_space` is of type `Discrete()` and `observation_space` is of type `Box()` which means it is continuous.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "Taking into account the considerations stated above some of the algorithms compatible with our environment are:\n",
    "\n",
    "- DQN\n",
    "- PPO\n",
    "- A2C\n",
    "\n",
    "The implementation for those algorithms will be provided by the Stable Baselines3 library, since it provides a very friendly and easy-to-use API, very handy for solving all sorts of tasks related to RL. The documentation for the library can be found [here](https://stable-baselines3.readthedocs.io/en/master/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs setup for visualization through TensorBoard\n",
    "import os\n",
    "\n",
    "logs_dir = f'logs'\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN (Deep Q-Network)\n",
    "\n",
    "The DQN algorithm is based in the Q-learning algorithm. Basically, the Q-table which store the Q-values for each pair `(state, action)` from the latter is substituted by a a neural network which is trained to estimate that same Q-value, in other words, Q-learning is for a discrete `observation_space` what DQN is for a continuous `observation_space`.\n",
    "\n",
    "Both DQN and Q-learning have the charateristic of being off-policy, meaning that the behaviour of the agent is completely independent from the produced estimates for the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "dqn_models_dir = 'models/dqn'\n",
    "if not os.path.exists(dqn_models_dir):\n",
    "    os.makedirs(dqn_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 0.1\n",
    "env = gym.make('gekitai-v0', render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=logs_dir)\n",
    "model.learn(total_timesteps=2e5, reset_num_timesteps=False, tb_log_name=f'dqn_v{version}')\n",
    "model.save(f'{dqn_models_dir}/gekitai_dnq_v{version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gekitai-v0', render_mode='human')\n",
    "observation = env.reset()\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _states = model.predict(observation, deterministic=True)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        env.render(mode='human')\n",
    "    \n",
    "    print(info)\n",
    "    observation = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}